{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "USE_GPU = True\n",
    "dtype = torch.float32 # We will be using float throughout this tutorial.\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "NUM_TRAIN = 40000\n",
    "\n",
    "# The torchvision.transforms package provides tools for preprocessing data\n",
    "# and for performing data augmentation; here we set up a transform to\n",
    "# preprocess the data by subtracting the mean RGB value and dividing by the\n",
    "# standard deviation of each RGB value; we've hardcoded the mean and std.\n",
    "transform = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            ])\n",
    "\n",
    "# We set up a Dataset object for each split (train / val / test); Datasets load\n",
    "# training examples one at a time, so we wrap each Dataset in a DataLoader which\n",
    "# iterates through the Dataset and forms minibatches. We divide the CIFAR-10\n",
    "# training set into train and val sets by passing a Sampler object to the\n",
    "# DataLoader telling how it should sample from the underlying Dataset.\n",
    "cifar10_train = dset.CIFAR10('../cs231n/datasets', train=True, download=True,\n",
    "                             transform=transform)\n",
    "loader_train = DataLoader(cifar10_train, batch_size=64,\n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "cifar10_val = dset.CIFAR10('../cs231n/datasets', train=True, download=True,\n",
    "                           transform=transform)\n",
    "loader_val = DataLoader(cifar10_val, batch_size=64,\n",
    "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
    "\n",
    "cifar10_test = dset.CIFAR10('../cs231n/datasets', train=False, download=True,\n",
    "                            transform=transform)\n",
    "loader_test = DataLoader(cifar10_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 40000\n",
      "val 10000\n",
      "test 10000\n"
     ]
    }
   ],
   "source": [
    "dataloaders = {}\n",
    "dataloaders['train'] = loader_train\n",
    "dataloaders['val']  = loader_val\n",
    "dataloaders['test'] = loader_test\n",
    "dataset_sizes = {}\n",
    "dataset_sizes['train'] = NUM_TRAIN\n",
    "dataset_sizes['val'] = cifar10_val.__len__() - NUM_TRAIN\n",
    "dataset_sizes['test'] = cifar10_test.__len__()\n",
    "\n",
    "for k,v in dataset_sizes.items():\n",
    "    print(k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,optimizer,criterion,num_epochs=1):\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "        print('-'*10)\n",
    "\n",
    "        for phase in ['train','val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs,labels)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                probs = nn.functional.softmax(outputs,dim=1)\n",
    "                pred_labels = torch.argmax(probs,dim=1)\n",
    "                running_corrects += torch.sum(pred_labels == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model):\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    running_corrects = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for inputs, labels in dataloaders['test']:\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(inputs)\n",
    "            probs = nn.functional.softmax(logits,dim=1)\n",
    "            pred_labels = torch.argmax(probs,dim=1)\n",
    "\n",
    "            running_corrects += torch.sum(pred_labels == labels)\n",
    "\n",
    "    print(running_corrects / dataset_sizes['test'])           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3,32,3,padding=1)\n",
    "        self.conv2 = nn.Conv2d(32,64,3,padding=1)\n",
    "        self.conv3 = nn.Conv2d(64,128,3,padding=1)\n",
    "        self.conv4 = nn.Conv2d(128,256,3,padding=1)\n",
    "        self.fc = nn.Linear(8*8*256,10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        scores = self.fc(x)\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n",
      "train Loss: 1.6236 Acc: 0.4687\n",
      "val Loss: 1.1400 Acc: 0.5987\n",
      "Epoch 2/10\n",
      "----------\n",
      "train Loss: 0.9710 Acc: 0.6590\n",
      "val Loss: 0.9146 Acc: 0.6785\n",
      "Epoch 3/10\n",
      "----------\n",
      "train Loss: 0.7801 Acc: 0.7292\n",
      "val Loss: 0.8544 Acc: 0.7123\n",
      "Epoch 4/10\n",
      "----------\n",
      "train Loss: 0.6441 Acc: 0.7787\n",
      "val Loss: 0.7306 Acc: 0.7529\n",
      "Epoch 5/10\n",
      "----------\n",
      "train Loss: 0.5456 Acc: 0.8122\n",
      "val Loss: 0.6539 Acc: 0.7801\n",
      "Epoch 6/10\n",
      "----------\n",
      "train Loss: 0.4577 Acc: 0.8418\n",
      "val Loss: 0.6503 Acc: 0.7827\n",
      "Epoch 7/10\n",
      "----------\n",
      "train Loss: 0.3699 Acc: 0.8749\n",
      "val Loss: 0.7214 Acc: 0.7797\n",
      "Epoch 8/10\n",
      "----------\n",
      "train Loss: 0.2896 Acc: 0.8992\n",
      "val Loss: 0.6094 Acc: 0.8095\n",
      "Epoch 9/10\n",
      "----------\n",
      "train Loss: 0.2210 Acc: 0.9232\n",
      "val Loss: 0.7512 Acc: 0.7829\n",
      "Epoch 10/10\n",
      "----------\n",
      "train Loss: 0.1668 Acc: 0.9425\n",
      "val Loss: 0.6635 Acc: 0.8093\n"
     ]
    }
   ],
   "source": [
    "model = MyModel()\n",
    "optimizer = optim.Adam(model.parameters(),weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_model(model, optimizer, criterion, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8005, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "test_model(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snowflakes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
